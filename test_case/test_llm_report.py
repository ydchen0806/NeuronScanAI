#!/usr/bin/env python3
"""
NeuroScan AI - LLM æŠ¥å‘Šç”Ÿæˆæµ‹è¯•
ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„ Ollama æ¨¡å‹ç”ŸæˆåŒ»å­¦æŠ¥å‘Š
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(__file__).parent / "reports_llm"
OUTPUT_DIR.mkdir(exist_ok=True)


def test_ollama_connection():
    """æµ‹è¯• Ollama è¿æ¥"""
    print("ğŸ”— æµ‹è¯• Ollama è¿æ¥...")
    
    try:
        import ollama
        models = ollama.list()
        print(f"   âœ… Ollama è¿æ¥æˆåŠŸ")
        print(f"   ğŸ“‹ å¯ç”¨æ¨¡å‹:")
        for model in models.get('models', []):
            name = model.get('name', 'unknown')
            size = model.get('size', 0) / (1024**3)  # GB
            print(f"      - {name} ({size:.1f} GB)")
        return True
    except Exception as e:
        print(f"   âŒ Ollama è¿æ¥å¤±è´¥: {e}")
        return False


def test_model_inference(model_name: str = "meditron:7b"):
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print(f"\nğŸ§  æµ‹è¯•æ¨¡å‹æ¨ç†: {model_name}")
    
    try:
        import ollama
        
        # ç®€å•çš„åŒ»å­¦é—®é¢˜æµ‹è¯•
        prompt = """ä½ æ˜¯ä¸€åæ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯·ç®€è¦æè¿°ä»¥ä¸‹ CT å½±åƒå‘ç°çš„ä¸´åºŠæ„ä¹‰ï¼š

å‘ç°ï¼šå³è‚ºä¸Šå¶åæ®µå¯è§ä¸€ä¸ª 12mm çš„éƒ¨åˆ†å®æ€§ç»“èŠ‚ï¼Œè¾¹ç•Œæ¸…æ™°ï¼Œå†…éƒ¨å¯è§ç£¨ç»ç’ƒæˆåˆ†ã€‚

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œé™åˆ¶åœ¨ 100 å­—ä»¥å†…ã€‚"""
        
        print("   å‘é€æµ‹è¯•è¯·æ±‚...")
        response = ollama.chat(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¯·ç”¨ä¸“ä¸šä½†ç®€æ´çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            options={
                "temperature": 0.1,
                "num_predict": 200
            }
        )
        
        result = response['message']['content']
        print(f"   âœ… æ¨¡å‹å“åº”æˆåŠŸ")
        print(f"\n   ğŸ“ æ¨¡å‹å›å¤:")
        print("   " + "-" * 50)
        for line in result.split('\n'):
            print(f"   {line}")
        print("   " + "-" * 50)
        
        return True
    except Exception as e:
        print(f"   âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_report_with_llm():
    """ä½¿ç”¨ LLM ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
    print("\nğŸ“„ ä½¿ç”¨ LLM ç”ŸæˆåŒ»å­¦æŠ¥å‘Š...")
    
    try:
        import ollama
        
        # æ¨¡æ‹Ÿçš„å½±åƒå‘ç°æ•°æ®
        findings_data = {
            "patient_id": "LEARN2REG_001",
            "study_date": "2026-01-24",
            "modality": "CT",
            "body_part": "èƒ¸éƒ¨",
            "findings": [
                {
                    "location": "å³è‚ºä¸Šå¶åæ®µ",
                    "type": "ç»“èŠ‚",
                    "size_mm": 12.5,
                    "density": "éƒ¨åˆ†å®æ€§",
                    "shape": "åˆ†å¶çŠ¶",
                    "margin": "è¾¹ç•Œæ¸…æ™°",
                    "calcification": "æ— ",
                    "enhancement": "æœªè¯„ä¼°"
                }
            ],
            "comparison": {
                "baseline_date": "2025-10-01",
                "baseline_size_mm": 10.0,
                "change_percent": 25.0,
                "interval_days": 115
            }
        }
        
        # æ„å»ºè¯¦ç»†çš„ prompt
        system_prompt = """ä½ æ˜¯ä¸€åèµ„æ·±çš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œä¸“é•¿äºèƒ¸éƒ¨ CT å½±åƒè¯Šæ–­ã€‚
è¯·æ ¹æ®æä¾›çš„å½±åƒæ•°æ®ç”Ÿæˆç¬¦åˆ ACR (ç¾å›½æ”¾å°„å­¦ä¼š) æ ‡å‡†çš„è¯Šæ–­æŠ¥å‘Šã€‚

æŠ¥å‘Šè¦æ±‚ï¼š
1. ä½¿ç”¨ä¸“ä¸šçš„åŒ»å­¦æœ¯è¯­
2. ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«ï¼šä¸´åºŠä¿¡æ¯ã€å½±åƒæ‰€è§ã€è¯Šæ–­å°è±¡ã€å»ºè®®
3. å¯¹äºçºµå‘å¯¹æ¯”ï¼Œéœ€è¦è¯„ä¼° RECIST 1.1 æ ‡å‡†
4. å»ºè®®è¦å…·ä½“å¯è¡Œ
5. ä½¿ç”¨ä¸­æ–‡æ’°å†™"""

        user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹å½±åƒæ•°æ®ç”Ÿæˆè¯Šæ–­æŠ¥å‘Šï¼š

æ‚£è€…ä¿¡æ¯:
- æ‚£è€… ID: {findings_data['patient_id']}
- æ£€æŸ¥æ—¥æœŸ: {findings_data['study_date']}
- æ£€æŸ¥ç±»å‹: {findings_data['modality']}
- æ£€æŸ¥éƒ¨ä½: {findings_data['body_part']}

å½±åƒå‘ç°:
{json.dumps(findings_data['findings'], indent=2, ensure_ascii=False)}

çºµå‘å¯¹æ¯”ä¿¡æ¯:
- åŸºçº¿æ£€æŸ¥æ—¥æœŸ: {findings_data['comparison']['baseline_date']}
- åŸºçº¿ç—…ç¶å¤§å°: {findings_data['comparison']['baseline_size_mm']} mm
- å½“å‰ç—…ç¶å¤§å°: {findings_data['findings'][0]['size_mm']} mm
- å˜åŒ–å¹…åº¦: {findings_data['comparison']['change_percent']}%
- æ£€æŸ¥é—´éš”: {findings_data['comparison']['interval_days']} å¤©

è¯·ç”Ÿæˆå®Œæ•´çš„è¯Šæ–­æŠ¥å‘Šï¼ŒåŒ…æ‹¬ RECIST 1.1 è¯„ä¼°ã€‚"""

        print("   å‘é€æŠ¥å‘Šç”Ÿæˆè¯·æ±‚...")
        print("   (è¿™å¯èƒ½éœ€è¦ 30-60 ç§’...)")
        
        response = ollama.chat(
            model="meditron:7b",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            options={
                "temperature": 0.1,
                "num_predict": 1500
            }
        )
        
        report_content = response['message']['content']
        
        print("   âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = OUTPUT_DIR / "llm_generated_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# NeuroScan AI - LLM ç”Ÿæˆçš„è¯Šæ–­æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ä½¿ç”¨æ¨¡å‹**: meditron:7b\n\n")
            f.write("---\n\n")
            f.write(report_content)
            f.write("\n\n---\n\n")
            f.write("*æœ¬æŠ¥å‘Šç”± NeuroScan AI ä½¿ç”¨æœ¬åœ° LLM ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ*\n")
        
        print(f"   ğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # ç”Ÿæˆ HTML ç‰ˆæœ¬
        html_report = generate_html_report(report_content, findings_data)
        html_path = OUTPUT_DIR / "llm_generated_report.html"
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_report)
        print(f"   ğŸ“ HTML æŠ¥å‘Š: {html_path}")
        
        # æ‰“å°æŠ¥å‘Šé¢„è§ˆ
        print(f"\n   ğŸ“ æŠ¥å‘Šé¢„è§ˆ:")
        print("   " + "=" * 60)
        preview = report_content[:800] + "..." if len(report_content) > 800 else report_content
        for line in preview.split('\n'):
            print(f"   {line}")
        print("   " + "=" * 60)
        
        return True
        
    except Exception as e:
        print(f"   âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_html_report(content: str, data: dict) -> str:
    """ç”Ÿæˆ HTML æ ¼å¼æŠ¥å‘Š"""
    
    # ç®€å•çš„ Markdown åˆ° HTML è½¬æ¢
    html_content = content.replace('\n', '<br>\n')
    html_content = html_content.replace('**', '<strong>').replace('**', '</strong>')
    
    html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeuroScan AI - LLM ç”ŸæˆæŠ¥å‘Š</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            color: #eee;
            min-height: 100vh;
            padding: 40px 20px;
        }}
        .container {{ max-width: 900px; margin: 0 auto; }}
        .header {{
            text-align: center;
            margin-bottom: 30px;
            padding: 30px;
            background: rgba(255,255,255,0.05);
            border-radius: 15px;
            border: 1px solid rgba(100, 255, 218, 0.2);
        }}
        .header h1 {{
            font-size: 2.2em;
            background: linear-gradient(90deg, #64ffda, #00d9ff);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 10px;
        }}
        .badge {{
            display: inline-block;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.85em;
            margin: 5px;
        }}
        .badge.llm {{ background: linear-gradient(90deg, #ff6b6b, #ffa500); color: #fff; }}
        .badge.model {{ background: rgba(100, 255, 218, 0.2); color: #64ffda; }}
        .content {{
            background: rgba(255,255,255,0.03);
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 20px;
            border: 1px solid rgba(100, 255, 218, 0.1);
            line-height: 1.8;
        }}
        .content h2 {{ color: #64ffda; margin: 20px 0 10px 0; }}
        .content h3 {{ color: #00d9ff; margin: 15px 0 8px 0; }}
        .meta {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-bottom: 20px;
        }}
        .meta-item {{
            background: rgba(100, 255, 218, 0.05);
            padding: 15px;
            border-radius: 10px;
        }}
        .meta-item label {{ color: #8892b0; font-size: 0.9em; }}
        .meta-item value {{ color: #64ffda; font-weight: 500; }}
        .footer {{
            text-align: center;
            padding: 20px;
            color: #666;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¥ NeuroScan AI</h1>
            <p>LLM ç”Ÿæˆçš„åŒ»å­¦è¯Šæ–­æŠ¥å‘Š</p>
            <div style="margin-top: 15px;">
                <span class="badge llm">ğŸ¤– AI ç”Ÿæˆ</span>
                <span class="badge model">meditron:7b</span>
            </div>
        </div>
        
        <div class="meta">
            <div class="meta-item">
                <label>æ‚£è€… ID</label><br>
                <value>{data['patient_id']}</value>
            </div>
            <div class="meta-item">
                <label>æ£€æŸ¥æ—¥æœŸ</label><br>
                <value>{data['study_date']}</value>
            </div>
            <div class="meta-item">
                <label>æ£€æŸ¥ç±»å‹</label><br>
                <value>{data['modality']} {data['body_part']}</value>
            </div>
            <div class="meta-item">
                <label>ç”Ÿæˆæ—¶é—´</label><br>
                <value>{datetime.now().strftime('%Y-%m-%d %H:%M')}</value>
            </div>
        </div>
        
        <div class="content">
            {html_content}
        </div>
        
        <div class="footer">
            <p>NeuroScan AI - æ™ºèƒ½åŒ»å­¦å½±åƒçºµå‘è¯Šæ–­ç³»ç»Ÿ</p>
            <p style="margin-top: 10px;">âš ï¸ æœ¬æŠ¥å‘Šç”± AI è¾…åŠ©ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºä¸´åºŠè¯Šæ–­ä¾æ®</p>
        </div>
    </div>
</body>
</html>"""
    return html


def compare_models():
    """å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ç”Ÿæˆæ•ˆæœ"""
    print("\nğŸ”¬ å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ç”Ÿæˆæ•ˆæœ...")
    
    models = ["llama3.1:8b", "meditron:7b", "medllama2:7b"]
    prompt = """ä½œä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¯·ç”¨ä¸€å¥è¯æè¿°ä»¥ä¸‹å‘ç°çš„ä¸´åºŠæ„ä¹‰ï¼š
å³è‚ºä¸Šå¶å¯è§ä¸€ä¸ª 12mm éƒ¨åˆ†å®æ€§ç»“èŠ‚ï¼Œè¾ƒ 3 ä¸ªæœˆå‰å¢å¤§ 25%ã€‚"""
    
    results = {}
    
    try:
        import ollama
        
        for model in models:
            print(f"\n   æµ‹è¯•æ¨¡å‹: {model}")
            try:
                response = ollama.chat(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¯·ç”¨ä¸­æ–‡ç®€æ´å›ç­”ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    options={"temperature": 0.1, "num_predict": 150}
                )
                results[model] = response['message']['content']
                print(f"   âœ… {model}: {results[model][:100]}...")
            except Exception as e:
                print(f"   âŒ {model}: å¤±è´¥ - {e}")
                results[model] = f"Error: {e}"
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison_path = OUTPUT_DIR / "model_comparison.md"
        with open(comparison_path, 'w', encoding='utf-8') as f:
            f.write("# LLM æ¨¡å‹å¯¹æ¯”æµ‹è¯•\n\n")
            f.write(f"**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**æµ‹è¯•é—®é¢˜**: {prompt}\n\n")
            f.write("---\n\n")
            for model, result in results.items():
                f.write(f"## {model}\n\n")
                f.write(f"{result}\n\n")
                f.write("---\n\n")
        
        print(f"\n   ğŸ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_path}")
        
    except Exception as e:
        print(f"   âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("NeuroScan AI - LLM æŠ¥å‘Šç”Ÿæˆæµ‹è¯•")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # 1. æµ‹è¯•è¿æ¥
    if not test_ollama_connection():
        print("\nâŒ Ollama æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨: ollama serve")
        return False
    
    # 2. æµ‹è¯•æ¨¡å‹æ¨ç†
    if not test_model_inference("meditron:7b"):
        print("\nâš ï¸ æ¨¡å‹æ¨ç†æµ‹è¯•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–æ¨¡å‹...")
        if not test_model_inference("llama3.1:8b"):
            return False
    
    # 3. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    generate_report_with_llm()
    
    # 4. å¯¹æ¯”ä¸åŒæ¨¡å‹
    compare_models()
    
    # 5. æ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… LLM æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å®Œæˆ!")
    print("=" * 60)
    print(f"\nğŸ“ æŠ¥å‘Šä½ç½®: {OUTPUT_DIR}")
    print("\nğŸŒ æŸ¥çœ‹æ–¹å¼:")
    print(f"   cd {OUTPUT_DIR} && python -m http.server 8892")
    print("   ç„¶åè®¿é—® http://localhost:8892/llm_generated_report.html")
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

