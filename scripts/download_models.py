#!/usr/bin/env python3
"""
æ¨¡å‹ä¸‹è½½è„šæœ¬
ä¸‹è½½ MONAI Bundle å’Œå…¶ä»–å¿…è¦çš„æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_models.py

éœ€è¦å…ˆè®¾ç½®ä»£ç†:
    export http_proxy=http://192.168.32.28:18000
    export https_proxy=http://192.168.32.28:18000
"""
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))


def download_monai_bundle():
    """ä¸‹è½½ MONAI Bundle åˆ†å‰²æ¨¡å‹"""
    print("=" * 60)
    print("MONAI Bundle æ¨¡å‹ä¸‹è½½")
    print("=" * 60)
    
    from monai.bundle import download
    
    models_dir = project_root / "models" / "monai_bundles"
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # è¦ä¸‹è½½çš„ bundle åˆ—è¡¨
    bundles = [
        "wholeBody_ct_segmentation",  # å…¨èº« CT åˆ†å‰²
    ]
    
    for bundle_name in bundles:
        bundle_path = models_dir / bundle_name
        
        if bundle_path.exists():
            print(f"âœ… {bundle_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            continue
        
        print(f"\nâ¬‡ï¸  ä¸‹è½½ {bundle_name}...")
        
        try:
            download(
                name=bundle_name,
                bundle_dir=str(models_dir),
                source="monaihosting"
            )
            print(f"âœ… {bundle_name} ä¸‹è½½å®Œæˆ")
        except Exception as e:
            print(f"âŒ {bundle_name} ä¸‹è½½å¤±è´¥: {e}")
    
    print("\n" + "=" * 60)
    print("æ¨¡å‹ä¸‹è½½å®Œæˆ!")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {models_dir}")
    print("=" * 60)


def download_llm_model():
    """
    ä¸‹è½½ LLM æ¨¡å‹ (å¦‚æœä½¿ç”¨æœ¬åœ°éƒ¨ç½²)
    è¿™é‡Œæä¾›ä½¿ç”¨ Hugging Face ä¸‹è½½çš„ç¤ºä¾‹
    """
    print("\n" + "=" * 60)
    print("LLM æ¨¡å‹é…ç½®è¯´æ˜")
    print("=" * 60)
    
    print("""
æœ¬ç³»ç»Ÿæ”¯æŒä»¥ä¸‹ LLM é…ç½®æ–¹å¼:

1. ä½¿ç”¨ OpenAI API å…¼å®¹æ¥å£ (æ¨è):
   - è®¾ç½®ç¯å¢ƒå˜é‡:
     export LLM_BASE_URL="http://your-llm-server:8000/v1"
     export LLM_API_KEY="your-api-key"
   
2. ä½¿ç”¨ vLLM æœ¬åœ°éƒ¨ç½²:
   - å®‰è£… vLLM: pip install vllm
   - å¯åŠ¨æœåŠ¡:
     python -m vllm.entrypoints.openai.api_server \\
       --model meta-llama/Llama-3-70B-Instruct \\
       --tensor-parallel-size 4

3. ä½¿ç”¨ Ollama:
   - å®‰è£… Ollama: curl -fsSL https://ollama.com/install.sh | sh
   - æ‹‰å–æ¨¡å‹: ollama pull llama3:70b
   - è®¾ç½®ç¯å¢ƒå˜é‡:
     export LLM_BASE_URL="http://localhost:11434/v1"

4. ä½¿ç”¨åŒ»ç–—ä¸“ç”¨æ¨¡å‹ Med42:
   - ä¸‹è½½: huggingface-cli download m42-health/med42-70b
   - ä½¿ç”¨ vLLM éƒ¨ç½²
""")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¨¡å‹ä¸‹è½½è„šæœ¬")
    parser.add_argument(
        "--model",
        choices=["monai", "llm", "all"],
        default="monai",
        help="è¦ä¸‹è½½çš„æ¨¡å‹ç±»å‹"
    )
    
    args = parser.parse_args()
    
    if args.model in ["monai", "all"]:
        download_monai_bundle()
    
    if args.model in ["llm", "all"]:
        download_llm_model()


if __name__ == "__main__":
    main()


