#!/usr/bin/env python3
"""
LLM éƒ¨ç½²å’Œé…ç½®è„šæœ¬

æ”¯æŒçš„ LLM åç«¯:
1. Ollama (æ¨èç”¨äºæœ¬åœ°éƒ¨ç½²)
2. vLLM (é«˜æ€§èƒ½æ¨ç†)
3. OpenAI å…¼å®¹ API

æ¨èæ¨¡å‹:
- llama3:8b-instruct (è½»é‡çº§ï¼Œé€‚åˆæµ‹è¯•)
- llama3:70b-instruct (PRD æ¨è)
- meditron:7b (åŒ»å­¦ä¸“ç”¨)
- med42-v2 (åŒ»å­¦ä¸“ç”¨)
"""

import os
import sys
import subprocess
import json
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def check_ollama():
    """æ£€æŸ¥ Ollama æ˜¯å¦å®‰è£…"""
    try:
        result = subprocess.run(
            ["ollama", "--version"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            print(f"âœ… Ollama å·²å®‰è£…: {result.stdout.strip()}")
            return True
    except FileNotFoundError:
        pass
    
    print("âŒ Ollama æœªå®‰è£…")
    return False


def install_ollama():
    """å®‰è£… Ollama"""
    print("\nğŸ“¦ å®‰è£… Ollama...")
    print("è¯·è®¿é—® https://ollama.ai ä¸‹è½½å®‰è£…")
    print("\næˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:")
    print("  curl -fsSL https://ollama.ai/install.sh | sh")
    
    return False


def list_ollama_models():
    """åˆ—å‡ºå·²å®‰è£…çš„ Ollama æ¨¡å‹"""
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            print("\nğŸ“‹ å·²å®‰è£…çš„æ¨¡å‹:")
            print(result.stdout)
            return result.stdout
    except:
        pass
    return ""


def pull_ollama_model(model_name: str):
    """ä¸‹è½½ Ollama æ¨¡å‹"""
    print(f"\nâ¬‡ï¸  ä¸‹è½½æ¨¡å‹: {model_name}")
    print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
    
    try:
        process = subprocess.Popen(
            ["ollama", "pull", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )
        
        for line in process.stdout:
            print(f"   {line.strip()}")
        
        process.wait()
        
        if process.returncode == 0:
            print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
            return True
        else:
            print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def test_ollama_model(model_name: str):
    """æµ‹è¯• Ollama æ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
    
    try:
        import ollama
        
        response = ollama.chat(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€åæ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"
                },
                {
                    "role": "user",
                    "content": "è¯·ç®€å•æè¿°è‚ºç»“èŠ‚çš„å½±åƒç‰¹å¾ã€‚"
                }
            ]
        )
        
        print(f"\nğŸ“ æ¨¡å‹å“åº”:")
        print("-" * 40)
        print(response['message']['content'])
        print("-" * 40)
        
        return True
    except ImportError:
        print("âŒ è¯·å®‰è£… ollama Python åŒ…: pip install ollama")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def update_config(llm_model: str, llm_backend: str = "ollama"):
    """æ›´æ–°é…ç½®æ–‡ä»¶"""
    env_path = PROJECT_ROOT / ".env"
    
    config = {
        "LLM_MODEL": llm_model,
        "LLM_BASE_URL": "http://localhost:11434/v1" if llm_backend == "ollama" else "http://localhost:8000/v1",
        "LLM_API_KEY": "ollama" if llm_backend == "ollama" else "local-key",
        "LLM_TEMPERATURE": "0.1",
        "LLM_MAX_TOKENS": "4096"
    }
    
    # è¯»å–ç°æœ‰é…ç½®
    existing_config = {}
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    existing_config[key] = value
    
    # æ›´æ–°é…ç½®
    existing_config.update(config)
    
    # å†™å…¥é…ç½®
    with open(env_path, 'w') as f:
        f.write("# NeuroScan AI é…ç½®\n\n")
        f.write("# LLM é…ç½®\n")
        for key, value in existing_config.items():
            f.write(f"{key}={value}\n")
    
    print(f"\nâœ… é…ç½®å·²æ›´æ–°: {env_path}")
    print(f"   LLM_MODEL={llm_model}")
    print(f"   LLM_BASE_URL={config['LLM_BASE_URL']}")


def test_report_generation():
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
    print("\nğŸ§ª æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ...")
    
    from app.services.report import ReportGenerator
    
    # ä½¿ç”¨æ¨¡æ¿æ¨¡å¼æµ‹è¯• (ä¸éœ€è¦ LLM)
    generator = ReportGenerator(llm_backend="template")
    
    # æµ‹è¯•å•æ¬¡æ‰«ææŠ¥å‘Š
    findings = [
        {
            "nodule_id": "nodule_1",
            "organ": "å³è‚ºä¸Šå¶",
            "location": "åæ®µ",
            "max_diameter_mm": 12.5,
            "volume_cc": 0.85,
            "mean_hu": 35.2,
            "shape": "åˆ†å¶çŠ¶",
            "density_type": "éƒ¨åˆ†å®æ€§"
        }
    ]
    
    report = generator.generate_single_report(
        patient_id="TEST_001",
        study_date="2026-01-24",
        body_part="èƒ¸éƒ¨",
        findings=findings,
        clinical_info="ä½“æ£€å‘ç°è‚ºç»“èŠ‚",
        modality="CT"
    )
    
    print("\nğŸ“„ å•æ¬¡æ‰«ææŠ¥å‘Š (æ¨¡æ¿æ¨¡å¼):")
    print("=" * 50)
    print(report[:1000] + "..." if len(report) > 1000 else report)
    
    # ä¿å­˜æŠ¥å‘Š
    output_dir = PROJECT_ROOT / "test_case" / "reports"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    report_path = generator.save_report(
        report,
        output_dir / "single_scan_report",
        format="html"
    )
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # æµ‹è¯•çºµå‘å¯¹æ¯”æŠ¥å‘Š
    baseline_findings = [
        {
            "nodule_id": "nodule_1",
            "organ": "å³è‚ºä¸Šå¶",
            "location": "åæ®µ",
            "max_diameter_mm": 10.0,
            "volume_cc": 0.52,
            "mean_hu": 32.0,
            "shape": "åœ†å½¢",
            "density_type": "å®æ€§"
        }
    ]
    
    followup_findings = [
        {
            "nodule_id": "nodule_1",
            "organ": "å³è‚ºä¸Šå¶",
            "location": "åæ®µ",
            "max_diameter_mm": 12.5,
            "volume_cc": 0.85,
            "mean_hu": 35.2,
            "shape": "åˆ†å¶çŠ¶",
            "density_type": "éƒ¨åˆ†å®æ€§"
        }
    ]
    
    longitudinal_report = generator.generate_longitudinal_report(
        patient_id="TEST_001",
        baseline_date="2025-10-01",
        followup_date="2026-01-24",
        baseline_findings=baseline_findings,
        followup_findings=followup_findings,
        registration_results={"mae_before": 432.5, "mae_after": 385.5},
        change_results={"diameter_change_pct": 25.0},
        modality="CT"
    )
    
    print("\nğŸ“„ çºµå‘å¯¹æ¯”æŠ¥å‘Š (æ¨¡æ¿æ¨¡å¼):")
    print("=" * 50)
    print(longitudinal_report[:1000] + "..." if len(longitudinal_report) > 1000 else longitudinal_report)
    
    report_path = generator.save_report(
        longitudinal_report,
        output_dir / "longitudinal_report",
        format="html"
    )
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("NeuroScan AI - LLM éƒ¨ç½²é…ç½®")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ Ollama
    has_ollama = check_ollama()
    
    if has_ollama:
        # åˆ—å‡ºå·²å®‰è£…æ¨¡å‹
        models = list_ollama_models()
        
        # æ¨èçš„åŒ»å­¦æ¨¡å‹
        recommended_models = [
            ("llama3:8b-instruct", "é€šç”¨æ¨¡å‹ï¼Œé€‚åˆæµ‹è¯•"),
            ("llama3.1:8b-instruct", "æœ€æ–°é€šç”¨æ¨¡å‹"),
            ("meditron:7b", "åŒ»å­¦ä¸“ç”¨æ¨¡å‹"),
            ("medllama2:7b", "åŒ»å­¦ä¸“ç”¨æ¨¡å‹"),
        ]
        
        print("\nğŸ’¡ æ¨èçš„æ¨¡å‹:")
        for model, desc in recommended_models:
            print(f"   - {model}: {desc}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
        if "llama3" in models.lower() or "meditron" in models.lower():
            print("\nâœ… å·²æœ‰å¯ç”¨çš„ LLM æ¨¡å‹")
        else:
            print("\nâš ï¸  å»ºè®®ä¸‹è½½ä¸€ä¸ªæ¨¡å‹:")
            print("   ollama pull llama3:8b-instruct")
    else:
        print("\nğŸ’¡ Ollama æ˜¯æ¨èçš„æœ¬åœ° LLM éƒ¨ç½²æ–¹æ¡ˆ")
        print("   å®‰è£…åå¯ä»¥è¿è¡Œ: ollama pull llama3:8b-instruct")
    
    # 2. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ (æ¨¡æ¿æ¨¡å¼)
    print("\n" + "=" * 60)
    print("æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ (æ¨¡æ¿æ¨¡å¼ - ä¸éœ€è¦ LLM)")
    print("=" * 60)
    
    try:
        test_report_generation()
        print("\nâœ… æŠ¥å‘Šç”Ÿæˆæµ‹è¯•æˆåŠŸ!")
    except Exception as e:
        print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # 3. æ€»ç»“
    print("\n" + "=" * 60)
    print("éƒ¨ç½²æ€»ç»“")
    print("=" * 60)
    
    print("""
ğŸ“‹ æŠ¥å‘Šç”Ÿæˆæ¨¡å—å·²å°±ç»ª!

æ”¯æŒçš„æ¨¡å¼:
1. æ¨¡æ¿æ¨¡å¼ (æ— éœ€ LLM): ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿ç”ŸæˆæŠ¥å‘Š
2. Ollama æ¨¡å¼: ä½¿ç”¨æœ¬åœ° LLM ç”Ÿæˆæ›´æ™ºèƒ½çš„æŠ¥å‘Š
3. vLLM/OpenAI æ¨¡å¼: ä½¿ç”¨é«˜æ€§èƒ½æ¨ç†æœåŠ¡

ä½¿ç”¨æ–¹æ³•:
```python
from app.services.report import ReportGenerator

# æ¨¡æ¿æ¨¡å¼
generator = ReportGenerator(llm_backend="template")

# Ollama æ¨¡å¼ (éœ€è¦å…ˆå®‰è£… Ollama å’Œæ¨¡å‹)
generator = ReportGenerator(llm_backend="ollama")

# ç”ŸæˆæŠ¥å‘Š
report = generator.generate_single_report(
    patient_id="P001",
    study_date="2026-01-24",
    body_part="èƒ¸éƒ¨",
    findings=[...],
    modality="CT"
)
```

å¦‚éœ€ä½¿ç”¨ LLM å¢å¼ºæŠ¥å‘Š:
1. å®‰è£… Ollama: curl -fsSL https://ollama.ai/install.sh | sh
2. ä¸‹è½½æ¨¡å‹: ollama pull llama3:8b-instruct
3. å¯åŠ¨æœåŠ¡: ollama serve
4. ä½¿ç”¨ Ollama æ¨¡å¼: ReportGenerator(llm_backend="ollama")
""")


if __name__ == "__main__":
    main()

