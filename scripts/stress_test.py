#!/usr/bin/env python3
"""
NeuroScan AI å¹¶å‘å‹åŠ›æµ‹è¯•
æµ‹è¯• CPU/GPU å³°å€¼ä½¿ç”¨æƒ…å†µï¼Œæ”¯æŒ 2-3 ä»»åŠ¡å¹¶å‘
"""

import os
import sys
import time
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from pathlib import Path
import psutil
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å…¨å±€ç›‘æ§æ•°æ®
monitor_data = {
    "cpu_percent": [],
    "memory_percent": [],
    "memory_gb": [],
    "gpu_memory_gb": [],
    "gpu_util": []
}
stop_monitor = False


def get_gpu_stats():
    """è·å–GPUçŠ¶æ€"""
    try:
        import torch
        if torch.cuda.is_available():
            # è·å–å½“å‰GPUçš„æ˜¾å­˜ä½¿ç”¨
            allocated = torch.cuda.memory_allocated() / (1024**3)
            reserved = torch.cuda.memory_reserved() / (1024**3)
            
            # ä½¿ç”¨nvidia-smiè·å–æ€»ä½“æ˜¾å­˜
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.used,utilization.gpu', '--format=csv,noheader,nounits', '-i', '0'],
                capture_output=True, text=True
            )
            if result.returncode == 0:
                parts = result.stdout.strip().split(',')
                mem_used = float(parts[0]) / 1024  # è½¬æ¢ä¸ºGB
                gpu_util = float(parts[1])
                return mem_used, gpu_util
            return allocated, 0
        return 0, 0
    except:
        return 0, 0


def resource_monitor(interval=0.5):
    """åå°èµ„æºç›‘æ§çº¿ç¨‹"""
    global stop_monitor, monitor_data
    
    while not stop_monitor:
        # CPU
        cpu_percent = psutil.cpu_percent(interval=None)
        monitor_data["cpu_percent"].append(cpu_percent)
        
        # å†…å­˜
        mem = psutil.virtual_memory()
        monitor_data["memory_percent"].append(mem.percent)
        monitor_data["memory_gb"].append(mem.used / (1024**3))
        
        # GPU
        gpu_mem, gpu_util = get_gpu_stats()
        monitor_data["gpu_memory_gb"].append(gpu_mem)
        monitor_data["gpu_util"].append(gpu_util)
        
        time.sleep(interval)


def run_single_pipeline(task_id, data_pair):
    """è¿è¡Œå•ä¸ªåˆ†ææµæ°´çº¿"""
    baseline_path, followup_path = data_pair
    
    print(f"  ğŸ”„ ä»»åŠ¡ {task_id}: å¼€å§‹å¤„ç† {Path(baseline_path).parent.name}")
    start_time = time.time()
    
    try:
        # å¯¼å…¥æ¨¡å—
        from app.services.dicom import DicomLoader
        from app.services.registration import ImageRegistrator
        from app.services.analysis import ChangeDetector
        
        loader = DicomLoader()
        registrator = ImageRegistrator()
        detector = ChangeDetector()
        
        # 1. åŠ è½½æ•°æ®
        t0 = time.time()
        baseline_data, _ = loader.load_nifti(baseline_path)
        followup_data, _ = loader.load_nifti(followup_path)
        load_time = time.time() - t0
        
        # 2. é…å‡†
        t0 = time.time()
        reg_result = registrator.register(followup_data, baseline_data, use_deformable=True)
        reg_time = time.time() - t0
        
        # 3. å˜åŒ–æ£€æµ‹
        t0 = time.time()
        change_result = detector.detect_changes(baseline_data, reg_result["warped_image"])
        detect_time = time.time() - t0
        
        total_time = time.time() - start_time
        
        return {
            "task_id": task_id,
            "status": "success",
            "load_time": load_time,
            "reg_time": reg_time,
            "detect_time": detect_time,
            "total_time": total_time,
            "data_shape": baseline_data.shape
        }
        
    except Exception as e:
        return {
            "task_id": task_id,
            "status": "error",
            "error": str(e),
            "total_time": time.time() - start_time
        }


def run_segmentation_task(task_id, nifti_path):
    """è¿è¡Œåˆ†å‰²ä»»åŠ¡ï¼ˆGPUå¯†é›†å‹ï¼‰"""
    print(f"  ğŸ§  åˆ†å‰²ä»»åŠ¡ {task_id}: å¼€å§‹å¤„ç†")
    start_time = time.time()
    
    try:
        import torch
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        
        from app.services.segmentation import OrganSegmentor
        segmentor = OrganSegmentor()
        
        # æ‰§è¡Œåˆ†å‰²
        from app.services.dicom import DicomLoader
        loader = DicomLoader()
        data, _ = loader.load_nifti(nifti_path)
        
        # åˆ†å‰²æ¨ç†
        result = segmentor.segment(data)
        
        total_time = time.time() - start_time
        
        # è®°å½•GPUå³°å€¼
        peak_mem = torch.cuda.max_memory_allocated() / (1024**3)
        
        return {
            "task_id": task_id,
            "status": "success",
            "total_time": total_time,
            "gpu_peak_gb": peak_mem
        }
        
    except Exception as e:
        return {
            "task_id": task_id,
            "status": "error",
            "error": str(e),
            "total_time": time.time() - start_time
        }


def get_test_data_pairs(data_dir, max_pairs=5):
    """è·å–æµ‹è¯•æ•°æ®å¯¹"""
    data_path = Path(data_dir) / "processed"
    pairs = []
    
    for case_dir in sorted(data_path.glob("real_lung_*"))[:max_pairs]:
        baseline = case_dir / "baseline.nii.gz"
        followup = case_dir / "followup.nii.gz"
        if baseline.exists() and followup.exists():
            pairs.append((str(baseline), str(followup)))
    
    return pairs


def print_stats(title, data_list):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    if not data_list:
        return
    arr = np.array(data_list)
    print(f"  {title}:")
    print(f"    å¹³å‡: {np.mean(arr):.2f}")
    print(f"    å³°å€¼: {np.max(arr):.2f}")
    print(f"    æœ€å°: {np.min(arr):.2f}")


def main():
    global stop_monitor, monitor_data
    
    print("=" * 70)
    print("ğŸ”¥ NeuroScan AI å¹¶å‘å‹åŠ›æµ‹è¯•")
    print("=" * 70)
    
    # ç³»ç»Ÿä¿¡æ¯
    print(f"\nğŸ“Š ç³»ç»Ÿé…ç½®:")
    print(f"  CPU æ ¸å¿ƒ: {psutil.cpu_count(logical=False)} ç‰©ç†æ ¸ / {psutil.cpu_count()} é€»è¾‘æ ¸")
    print(f"  æ€»å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    
    try:
        import torch
        if torch.cuda.is_available():
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    except:
        print("  GPU: ä¸å¯ç”¨")
    
    # è·å–æµ‹è¯•æ•°æ®
    data_dir = Path(__file__).parent.parent / "data"
    pairs = get_test_data_pairs(data_dir, max_pairs=5)
    
    if len(pairs) < 2:
        print("\nâŒ æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ 2 å¯¹æ•°æ®")
        print("   è¯·å…ˆè¿è¡Œ: python scripts/download_datasets.py --dataset learn2reg")
        return
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(pairs)} å¯¹æµ‹è¯•æ•°æ®")
    
    # ========================================
    # æµ‹è¯• 1: å•ä»»åŠ¡åŸºå‡†
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“Œ æµ‹è¯• 1: å•ä»»åŠ¡åŸºå‡†æµ‹è¯•")
    print("=" * 70)
    
    monitor_data = {k: [] for k in monitor_data}
    stop_monitor = False
    
    # å¯åŠ¨ç›‘æ§
    monitor_thread = threading.Thread(target=resource_monitor, args=(0.2,))
    monitor_thread.start()
    
    result = run_single_pipeline(1, pairs[0])
    
    stop_monitor = True
    monitor_thread.join()
    
    if result["status"] == "success":
        print(f"\n  âœ… å•ä»»åŠ¡å®Œæˆ:")
        print(f"    åŠ è½½æ—¶é—´: {result['load_time']:.2f}s")
        print(f"    é…å‡†æ—¶é—´: {result['reg_time']:.2f}s")
        print(f"    æ£€æµ‹æ—¶é—´: {result['detect_time']:.2f}s")
        print(f"    æ€»æ—¶é—´: {result['total_time']:.2f}s")
    
    print(f"\n  ğŸ“ˆ å•ä»»åŠ¡èµ„æºå³°å€¼:")
    print(f"    CPU å³°å€¼: {max(monitor_data['cpu_percent']):.1f}%")
    print(f"    å†…å­˜å³°å€¼: {max(monitor_data['memory_gb']):.1f} GB ({max(monitor_data['memory_percent']):.1f}%)")
    print(f"    GPUæ˜¾å­˜å³°å€¼: {max(monitor_data['gpu_memory_gb']):.2f} GB")
    
    single_task_time = result["total_time"]
    single_cpu_peak = max(monitor_data['cpu_percent'])
    single_mem_peak = max(monitor_data['memory_gb'])
    
    # ========================================
    # æµ‹è¯• 2: 2 ä»»åŠ¡å¹¶å‘
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“Œ æµ‹è¯• 2: 2 ä»»åŠ¡å¹¶å‘å‹åŠ›æµ‹è¯•")
    print("=" * 70)
    
    monitor_data = {k: [] for k in monitor_data}
    stop_monitor = False
    
    monitor_thread = threading.Thread(target=resource_monitor, args=(0.2,))
    monitor_thread.start()
    
    start_time = time.time()
    results = []
    
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = []
        for i, pair in enumerate(pairs[:2]):
            futures.append(executor.submit(run_single_pipeline, i+1, pair))
        
        for future in as_completed(futures):
            results.append(future.result())
    
    concurrent_2_time = time.time() - start_time
    
    stop_monitor = True
    monitor_thread.join()
    
    success_count = sum(1 for r in results if r["status"] == "success")
    print(f"\n  âœ… 2ä»»åŠ¡å¹¶å‘å®Œæˆ: {success_count}/2 æˆåŠŸ")
    print(f"    æ€»è€—æ—¶: {concurrent_2_time:.2f}s")
    print(f"    å¹¶è¡Œæ•ˆç‡: {(single_task_time * 2 / concurrent_2_time * 100):.1f}%")
    
    print(f"\n  ğŸ“ˆ 2ä»»åŠ¡å¹¶å‘èµ„æºå³°å€¼:")
    print(f"    CPU å³°å€¼: {max(monitor_data['cpu_percent']):.1f}%")
    print(f"    å†…å­˜å³°å€¼: {max(monitor_data['memory_gb']):.1f} GB ({max(monitor_data['memory_percent']):.1f}%)")
    print(f"    GPUæ˜¾å­˜å³°å€¼: {max(monitor_data['gpu_memory_gb']):.2f} GB")
    
    concurrent_2_cpu = max(monitor_data['cpu_percent'])
    concurrent_2_mem = max(monitor_data['memory_gb'])
    
    # ========================================
    # æµ‹è¯• 3: 3 ä»»åŠ¡å¹¶å‘
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“Œ æµ‹è¯• 3: 3 ä»»åŠ¡å¹¶å‘å‹åŠ›æµ‹è¯•")
    print("=" * 70)
    
    if len(pairs) < 3:
        print("  âš ï¸ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡ 3 ä»»åŠ¡æµ‹è¯•")
    else:
        monitor_data = {k: [] for k in monitor_data}
        stop_monitor = False
        
        monitor_thread = threading.Thread(target=resource_monitor, args=(0.2,))
        monitor_thread.start()
        
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for i, pair in enumerate(pairs[:3]):
                futures.append(executor.submit(run_single_pipeline, i+1, pair))
            
            for future in as_completed(futures):
                results.append(future.result())
        
        concurrent_3_time = time.time() - start_time
        
        stop_monitor = True
        monitor_thread.join()
        
        success_count = sum(1 for r in results if r["status"] == "success")
        print(f"\n  âœ… 3ä»»åŠ¡å¹¶å‘å®Œæˆ: {success_count}/3 æˆåŠŸ")
        print(f"    æ€»è€—æ—¶: {concurrent_3_time:.2f}s")
        print(f"    å¹¶è¡Œæ•ˆç‡: {(single_task_time * 3 / concurrent_3_time * 100):.1f}%")
        
        print(f"\n  ğŸ“ˆ 3ä»»åŠ¡å¹¶å‘èµ„æºå³°å€¼:")
        print(f"    CPU å³°å€¼: {max(monitor_data['cpu_percent']):.1f}%")
        print(f"    å†…å­˜å³°å€¼: {max(monitor_data['memory_gb']):.1f} GB ({max(monitor_data['memory_percent']):.1f}%)")
        print(f"    GPUæ˜¾å­˜å³°å€¼: {max(monitor_data['gpu_memory_gb']):.2f} GB")
        
        concurrent_3_cpu = max(monitor_data['cpu_percent'])
        concurrent_3_mem = max(monitor_data['memory_gb'])
    
    # ========================================
    # æµ‹è¯• 4: GPU åˆ†å‰²ä»»åŠ¡ (å¯é€‰)
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“Œ æµ‹è¯• 4: GPU åˆ†å‰²ä»»åŠ¡å³°å€¼æµ‹è¯•")
    print("=" * 70)
    
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            
            monitor_data = {k: [] for k in monitor_data}
            stop_monitor = False
            
            monitor_thread = threading.Thread(target=resource_monitor, args=(0.2,))
            monitor_thread.start()
            
            # è¿è¡Œåˆ†å‰²
            seg_result = run_segmentation_task(1, pairs[0][0])
            
            stop_monitor = True
            monitor_thread.join()
            
            if seg_result["status"] == "success":
                print(f"\n  âœ… åˆ†å‰²ä»»åŠ¡å®Œæˆ:")
                print(f"    è€—æ—¶: {seg_result['total_time']:.2f}s")
                print(f"    GPUå³°å€¼: {seg_result.get('gpu_peak_gb', max(monitor_data['gpu_memory_gb'])):.2f} GB")
            else:
                print(f"\n  âš ï¸ åˆ†å‰²ä»»åŠ¡è·³è¿‡: {seg_result.get('error', 'unknown')}")
            
            print(f"\n  ğŸ“ˆ åˆ†å‰²ä»»åŠ¡èµ„æºå³°å€¼:")
            print(f"    CPU å³°å€¼: {max(monitor_data['cpu_percent']):.1f}%")
            print(f"    å†…å­˜å³°å€¼: {max(monitor_data['memory_gb']):.1f} GB")
            print(f"    GPUæ˜¾å­˜å³°å€¼: {max(monitor_data['gpu_memory_gb']):.2f} GB")
            
            gpu_seg_peak = max(monitor_data['gpu_memory_gb'])
        else:
            print("  âš ï¸ GPU ä¸å¯ç”¨ï¼Œè·³è¿‡åˆ†å‰²æµ‹è¯•")
            gpu_seg_peak = 0
    except Exception as e:
        print(f"  âš ï¸ åˆ†å‰²æµ‹è¯•å¤±è´¥: {e}")
        gpu_seg_peak = 0
    
    # ========================================
    # æœ€ç»ˆæŠ¥å‘Š
    # ========================================
    print("\n" + "=" * 70)
    print("ğŸ“‹ å‹åŠ›æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 70)
    
    print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NeuroScan AI èµ„æºéœ€æ±‚æŠ¥å‘Š                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     æµ‹è¯•åœºæ™¯     â”‚   CPU å³°å€¼    â”‚   å†…å­˜å³°å€¼    â”‚    GPU æ˜¾å­˜å³°å€¼   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å•ä»»åŠ¡é…å‡†      â”‚  {single_cpu_peak:>6.1f}%      â”‚  {single_mem_peak:>6.1f} GB    â”‚     ~0 GB (CPU)   â”‚
â”‚  2ä»»åŠ¡å¹¶å‘       â”‚  {concurrent_2_cpu:>6.1f}%      â”‚  {concurrent_2_mem:>6.1f} GB    â”‚     ~0 GB (CPU)   â”‚
â”‚  3ä»»åŠ¡å¹¶å‘       â”‚  {concurrent_3_cpu if 'concurrent_3_cpu' in dir() else 0:>6.1f}%      â”‚  {concurrent_3_mem if 'concurrent_3_mem' in dir() else 0:>6.1f} GB    â”‚     ~0 GB (CPU)   â”‚
â”‚  GPUåˆ†å‰²ä»»åŠ¡     â”‚   ~50%        â”‚  ~8 GB        â”‚   {gpu_seg_peak:>6.1f} GB       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         æ¨èç¡¬ä»¶é…ç½®                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æœ€ä½é…ç½® (å•ä»»åŠ¡): 4æ ¸ CPU, 8GB å†…å­˜, æ— éœ€GPU                       â”‚
â”‚  æ ‡å‡†é…ç½® (2å¹¶å‘):  8æ ¸ CPU, 16GB å†…å­˜, 12GB GPU (å¯é€‰)              â”‚
â”‚  æ¨èé…ç½® (3å¹¶å‘):  16æ ¸ CPU, 32GB å†…å­˜, 24GB GPU                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    print("âœ… å‹åŠ›æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()

