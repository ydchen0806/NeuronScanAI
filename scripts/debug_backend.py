#!/usr/bin/env python3
"""
NeuroScan AI åç«¯è°ƒè¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºç‹¬ç«‹æµ‹è¯•åç«¯çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. DICOM/NIfTI åŠ è½½
2. å›¾åƒé…å‡†ï¼ˆåˆšæ€§ + éåˆšæ€§ï¼‰
3. å˜åŒ–æ£€æµ‹
4. æŠ¥å‘Šç”Ÿæˆï¼ˆæ¨¡æ¿æ¨¡å¼ + LLM æ¨¡å¼ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python scripts/debug_backend.py

ä½œè€…: NeuroScan AI Team
æ—¥æœŸ: 2026-01-28
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import nibabel as nib
from datetime import datetime
import json
import traceback

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONIOENCODING'] = 'utf-8'


def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_step(step: str, status: str = "running"):
    """æ‰“å°æ­¥éª¤"""
    symbols = {
        "running": "ğŸ”„",
        "success": "âœ…",
        "error": "âŒ",
        "info": "â„¹ï¸",
        "warning": "âš ï¸"
    }
    print(f"\n{symbols.get(status, 'â€¢')} {step}")


def print_dict(d: dict, indent: int = 2):
    """æ ¼å¼åŒ–æ‰“å°å­—å…¸"""
    for k, v in d.items():
        if isinstance(v, float):
            print(f"{' ' * indent}{k}: {v:.4f}")
        else:
            print(f"{' ' * indent}{k}: {v}")


class BackendDebugger:
    """åç«¯è°ƒè¯•å™¨"""
    
    def __init__(self):
        self.data_dir = project_root / "data" / "processed"
        self.output_dir = project_root / "output" / "debug_results"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æµ‹è¯•ç»“æœ
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "tests": {}
        }
    
    def find_test_data(self):
        """æŸ¥æ‰¾å¯ç”¨çš„æµ‹è¯•æ•°æ®"""
        print_step("æŸ¥æ‰¾æµ‹è¯•æ•°æ®", "running")
        
        # æŸ¥æ‰¾ real_lung æ•°æ®
        lung_dirs = list(self.data_dir.glob("real_lung_*"))
        
        if lung_dirs:
            print(f"  æ‰¾åˆ° {len(lung_dirs)} ä¸ªè‚ºéƒ¨ CT æ•°æ®é›†:")
            for d in lung_dirs:
                files = list(d.glob("*.nii.gz"))
                print(f"    - {d.name}: {len(files)} ä¸ªæ–‡ä»¶")
            return lung_dirs[0]  # è¿”å›ç¬¬ä¸€ä¸ª
        
        # æŸ¥æ‰¾å…¶ä»–æ•°æ®
        all_dirs = [d for d in self.data_dir.iterdir() if d.is_dir()]
        if all_dirs:
            print(f"  æ‰¾åˆ° {len(all_dirs)} ä¸ªæ•°æ®ç›®å½•")
            return all_dirs[0]
        
        return None
    
    def test_dicom_loader(self):
        """æµ‹è¯• DICOM/NIfTI åŠ è½½å™¨"""
        print_header("æµ‹è¯• 1: DICOM/NIfTI åŠ è½½å™¨")
        
        try:
            from app.services.dicom import DicomLoader
            
            loader = DicomLoader()
            print_step("DicomLoader åˆå§‹åŒ–æˆåŠŸ", "success")
            
            # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
            test_dir = self.find_test_data()
            if test_dir is None:
                print_step("æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡åŠ è½½æµ‹è¯•", "warning")
                return None, None
            
            # åŠ è½½ NIfTI æ–‡ä»¶
            baseline_path = test_dir / "baseline.nii.gz"
            followup_path = test_dir / "followup.nii.gz"
            
            if not baseline_path.exists():
                # å°è¯•å…¶ä»–æ–‡ä»¶å
                nii_files = list(test_dir.glob("*.nii.gz"))
                if len(nii_files) >= 2:
                    baseline_path = nii_files[0]
                    followup_path = nii_files[1]
                elif len(nii_files) == 1:
                    baseline_path = nii_files[0]
                    followup_path = nii_files[0]  # ä½¿ç”¨åŒä¸€ä¸ªæ–‡ä»¶è¿›è¡Œæµ‹è¯•
            
            print_step(f"åŠ è½½åŸºçº¿: {baseline_path.name}", "running")
            baseline_data, baseline_img = loader.load_nifti(baseline_path)
            print(f"    å½¢çŠ¶: {baseline_data.shape}")
            print(f"    ä½“ç´ å¤§å°: {baseline_img.header.get_zooms()[:3]}")
            print(f"    æ•°æ®èŒƒå›´: [{baseline_data.min():.1f}, {baseline_data.max():.1f}]")
            
            print_step(f"åŠ è½½éšè®¿: {followup_path.name}", "running")
            followup_data, followup_img = loader.load_nifti(followup_path)
            print(f"    å½¢çŠ¶: {followup_data.shape}")
            print(f"    ä½“ç´ å¤§å°: {followup_img.header.get_zooms()[:3]}")
            
            self.results["tests"]["dicom_loader"] = {
                "status": "success",
                "baseline_shape": list(baseline_data.shape),
                "followup_shape": list(followup_data.shape)
            }
            
            print_step("DICOM/NIfTI åŠ è½½æµ‹è¯•é€šè¿‡", "success")
            return (baseline_path, baseline_data, baseline_img), (followup_path, followup_data, followup_img)
            
        except Exception as e:
            print_step(f"åŠ è½½æµ‹è¯•å¤±è´¥: {e}", "error")
            traceback.print_exc()
            self.results["tests"]["dicom_loader"] = {
                "status": "error",
                "error": str(e)
            }
            return None, None
    
    def test_registration(self, baseline_path: Path, followup_path: Path):
        """æµ‹è¯•å›¾åƒé…å‡†"""
        print_header("æµ‹è¯• 2: å›¾åƒé…å‡†")
        
        try:
            from app.services.registration import ImageRegistrator
            
            print_step("åˆå§‹åŒ–é…å‡†å™¨", "running")
            registrator = ImageRegistrator()
            print_step("ImageRegistrator åˆå§‹åŒ–æˆåŠŸ", "success")
            
            print_step("æ‰§è¡Œä¸¤çº§é…å‡†ï¼ˆåˆšæ€§ + éåˆšæ€§ï¼‰", "running")
            print("    è¿™å¯èƒ½éœ€è¦ 30-60 ç§’...")
            
            import time
            start_time = time.time()
            
            warped_path, transforms = registrator.register_files(
                followup_path,  # fixed
                baseline_path,  # moving
                use_deformable=True
            )
            
            elapsed = time.time() - start_time
            
            print(f"    é…å‡†å®Œæˆï¼è€—æ—¶: {elapsed:.1f} ç§’")
            print(f"    è¾“å‡ºæ–‡ä»¶: {warped_path}")
            print(f"    å˜æ¢ç±»å‹: {list(transforms.keys())}")
            
            self.results["tests"]["registration"] = {
                "status": "success",
                "elapsed_seconds": elapsed,
                "warped_path": str(warped_path),
                "transforms": list(transforms.keys())
            }
            
            print_step("é…å‡†æµ‹è¯•é€šè¿‡", "success")
            return warped_path
            
        except Exception as e:
            print_step(f"é…å‡†æµ‹è¯•å¤±è´¥: {e}", "error")
            traceback.print_exc()
            self.results["tests"]["registration"] = {
                "status": "error",
                "error": str(e)
            }
            return None
    
    def test_change_detection(self, followup_data: np.ndarray, warped_path: Path):
        """æµ‹è¯•å˜åŒ–æ£€æµ‹"""
        print_header("æµ‹è¯• 3: å˜åŒ–æ£€æµ‹")
        
        try:
            from app.services.analysis import ChangeDetector
            from app.services.dicom import DicomLoader
            
            print_step("åˆå§‹åŒ–å˜åŒ–æ£€æµ‹å™¨", "running")
            detector = ChangeDetector()
            loader = DicomLoader()
            
            # åŠ è½½é…å‡†åçš„å›¾åƒ
            warped_data, warped_img = loader.load_nifti(warped_path)
            spacing = tuple(warped_img.header.get_zooms()[:3])
            
            print_step("è®¡ç®—å·®åˆ†å›¾", "running")
            diff_map, significant = detector.compute_difference_map(
                followup_data, 
                warped_data
            )
            
            print(f"    å·®åˆ†å›¾èŒƒå›´: [{diff_map.min():.1f}, {diff_map.max():.1f}]")
            print(f"    æ˜¾è‘—å˜åŒ–ä½“ç´ : {(significant != 0).sum():,}")
            
            print_step("é‡åŒ–å˜åŒ–", "running")
            changes = detector.quantify_changes(diff_map, significant, spacing=spacing)
            print_dict(changes)
            
            # ç”Ÿæˆçƒ­åŠ›å›¾
            print_step("ç”Ÿæˆçƒ­åŠ›å›¾", "running")
            heatmap_path = self.output_dir / "diff_heatmap.png"
            detector.generate_heatmap(significant, followup_data, heatmap_path)
            print(f"    çƒ­åŠ›å›¾ä¿å­˜è‡³: {heatmap_path}")
            
            self.results["tests"]["change_detection"] = {
                "status": "success",
                "changes": {k: float(v) if isinstance(v, (np.floating, np.integer)) else v 
                           for k, v in changes.items()},
                "heatmap_path": str(heatmap_path)
            }
            
            print_step("å˜åŒ–æ£€æµ‹æµ‹è¯•é€šè¿‡", "success")
            return changes, significant
            
        except Exception as e:
            print_step(f"å˜åŒ–æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}", "error")
            traceback.print_exc()
            self.results["tests"]["change_detection"] = {
                "status": "error",
                "error": str(e)
            }
            return None, None
    
    def test_report_generation(self, change_results: dict = None):
        """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
        print_header("æµ‹è¯• 4: æŠ¥å‘Šç”Ÿæˆ")
        
        try:
            from app.services.report import ReportGenerator
            
            # æµ‹è¯•æ¨¡æ¿æ¨¡å¼
            print_step("æµ‹è¯•æ¨¡æ¿æ¨¡å¼æŠ¥å‘Šç”Ÿæˆ", "running")
            generator = ReportGenerator(llm_backend="template")
            
            # æ„é€ æµ‹è¯•æ•°æ®
            baseline_findings = [{
                "organ": "å³è‚ºä¸Šå¶",
                "location": "å‰æ®µ",
                "max_diameter_mm": 15.5,
                "volume_cc": 1.2,
                "mean_hu": -25,
                "shape": "ç±»åœ†å½¢",
                "density_type": "ç£¨ç»ç’ƒ"
            }]
            
            followup_findings = [{
                "organ": "å³è‚ºä¸Šå¶",
                "location": "å‰æ®µ",
                "max_diameter_mm": 12.3,
                "volume_cc": 0.8,
                "mean_hu": -20,
                "shape": "ç±»åœ†å½¢",
                "density_type": "ç£¨ç»ç’ƒ"
            }]
            
            registration_results = {
                "rigid": "completed",
                "deformable": "completed",
                "spacing": (1.0, 1.0, 1.0)
            }
            
            if change_results is None:
                change_results = {
                    "changed_voxels": 15000,
                    "change_percent": 0.05,
                    "max_hu_increase": 50.0,
                    "max_hu_decrease": -45.0
                }
            
            # ç”Ÿæˆçºµå‘æŠ¥å‘Š
            report = generator.generate_longitudinal_report(
                patient_id="TEST001",
                baseline_date="2025-06-15",
                followup_date="2026-01-28",
                baseline_findings=baseline_findings,
                followup_findings=followup_findings,
                registration_results=registration_results,
                change_results=change_results,
                modality="CT"
            )
            
            # ä¿å­˜æŠ¥å‘Š
            report_path = self.output_dir / "test_report.md"
            generator.save_report(report, report_path, format="md")
            print(f"    æ¨¡æ¿æŠ¥å‘Šä¿å­˜è‡³: {report_path}")
            
            # ç”Ÿæˆ HTML æŠ¥å‘Š
            html_path = self.output_dir / "test_report"
            generator.save_report(report, html_path, format="html")
            print(f"    HTML æŠ¥å‘Šä¿å­˜è‡³: {html_path}.html")
            
            # æµ‹è¯• LLM æ¨¡å¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            print_step("æµ‹è¯• LLM æ¨¡å¼æŠ¥å‘Šç”Ÿæˆ", "running")
            try:
                llm_generator = ReportGenerator(llm_backend="ollama")
                
                llm_report = llm_generator.generate_longitudinal_report(
                    patient_id="TEST001",
                    baseline_date="2025-06-15",
                    followup_date="2026-01-28",
                    baseline_findings=baseline_findings,
                    followup_findings=followup_findings,
                    registration_results=registration_results,
                    change_results=change_results,
                    modality="CT"
                )
                
                llm_report_path = self.output_dir / "test_report_llm.md"
                llm_generator.save_report(llm_report, llm_report_path, format="md")
                print(f"    LLM æŠ¥å‘Šä¿å­˜è‡³: {llm_report_path}")
                print_step("LLM æ¨¡å¼æµ‹è¯•é€šè¿‡", "success")
                
                self.results["tests"]["report_generation"] = {
                    "status": "success",
                    "template_report_path": str(report_path),
                    "llm_report_path": str(llm_report_path),
                    "llm_available": True
                }
                
            except Exception as llm_error:
                print_step(f"LLM æ¨¡å¼ä¸å¯ç”¨: {llm_error}", "warning")
                self.results["tests"]["report_generation"] = {
                    "status": "success",
                    "template_report_path": str(report_path),
                    "llm_available": False,
                    "llm_error": str(llm_error)
                }
            
            print_step("æŠ¥å‘Šç”Ÿæˆæµ‹è¯•é€šè¿‡", "success")
            return report_path
            
        except Exception as e:
            print_step(f"æŠ¥å‘Šç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}", "error")
            traceback.print_exc()
            self.results["tests"]["report_generation"] = {
                "status": "error",
                "error": str(e)
            }
            return None
    
    def test_segmentation(self, nifti_path: Path):
        """æµ‹è¯•å™¨å®˜åˆ†å‰²ï¼ˆå¯é€‰ï¼Œè€—æ—¶è¾ƒé•¿ï¼‰"""
        print_header("æµ‹è¯• 5: å™¨å®˜åˆ†å‰² (å¯é€‰)")
        
        # è¯¢é—®æ˜¯å¦è¿è¡Œ
        print("å™¨å®˜åˆ†å‰²éœ€è¦ GPU ä¸”è€—æ—¶è¾ƒé•¿ï¼ˆçº¦ 1-2 åˆ†é’Ÿï¼‰")
        
        try:
            from app.services.segmentation import OrganSegmentor
            
            print_step("åˆå§‹åŒ–åˆ†å‰²å™¨", "running")
            segmentor = OrganSegmentor()
            
            print_step("æ£€æŸ¥ GPU çŠ¶æ€", "running")
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"    GPU: {gpu_name}")
                print(f"    æ˜¾å­˜: {gpu_mem:.1f} GB")
                
                # æ£€æŸ¥å¯ç”¨æ˜¾å­˜
                free_mem = (torch.cuda.get_device_properties(0).total_memory - 
                           torch.cuda.memory_allocated()) / 1e9
                print(f"    å¯ç”¨æ˜¾å­˜: {free_mem:.1f} GB")
                
                if free_mem < 4.0:
                    print_step("æ˜¾å­˜ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘ 4GBï¼‰ï¼Œè·³è¿‡åˆ†å‰²æµ‹è¯•", "warning")
                    self.results["tests"]["segmentation"] = {
                        "status": "skipped",
                        "reason": "insufficient GPU memory"
                    }
                    return None
            else:
                print_step("GPU ä¸å¯ç”¨ï¼Œè·³è¿‡åˆ†å‰²æµ‹è¯•", "warning")
                self.results["tests"]["segmentation"] = {
                    "status": "skipped",
                    "reason": "GPU not available"
                }
                return None
            
            print_step("æ‰§è¡Œå™¨å®˜åˆ†å‰²", "running")
            print("    è¿™å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿ...")
            
            import time
            start_time = time.time()
            
            seg_path, organ_paths = segmentor.segment_file(
                nifti_path,
                save_individual_organs=False
            )
            
            elapsed = time.time() - start_time
            
            print(f"    åˆ†å‰²å®Œæˆï¼è€—æ—¶: {elapsed:.1f} ç§’")
            print(f"    åˆ†å‰²ç»“æœ: {seg_path}")
            
            self.results["tests"]["segmentation"] = {
                "status": "success",
                "elapsed_seconds": elapsed,
                "seg_path": str(seg_path)
            }
            
            print_step("åˆ†å‰²æµ‹è¯•é€šè¿‡", "success")
            return seg_path
            
        except Exception as e:
            print_step(f"åˆ†å‰²æµ‹è¯•å¤±è´¥: {e}", "error")
            traceback.print_exc()
            self.results["tests"]["segmentation"] = {
                "status": "error",
                "error": str(e)
            }
            return None
    
    def run_all_tests(self, skip_segmentation: bool = True):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print_header("NeuroScan AI åç«¯è°ƒè¯•")
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æµ‹è¯• 1: åŠ è½½å™¨
        baseline_result, followup_result = self.test_dicom_loader()
        
        if baseline_result is None or followup_result is None:
            print_step("æ— æ³•ç»§ç»­æµ‹è¯•ï¼Œéœ€è¦æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®", "error")
            self.save_results()
            return
        
        baseline_path, baseline_data, baseline_img = baseline_result
        followup_path, followup_data, followup_img = followup_result
        
        # æµ‹è¯• 2: é…å‡†
        warped_path = self.test_registration(baseline_path, followup_path)
        
        # æµ‹è¯• 3: å˜åŒ–æ£€æµ‹
        if warped_path:
            change_results, significant = self.test_change_detection(followup_data, warped_path)
        else:
            change_results = None
        
        # æµ‹è¯• 4: æŠ¥å‘Šç”Ÿæˆ
        self.test_report_generation(change_results)
        
        # æµ‹è¯• 5: åˆ†å‰²ï¼ˆå¯é€‰ï¼‰
        if not skip_segmentation:
            self.test_segmentation(baseline_path)
        else:
            print_step("è·³è¿‡åˆ†å‰²æµ‹è¯•ï¼ˆä½¿ç”¨ --with-segmentation å¯ç”¨ï¼‰", "info")
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
    
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        results_path = self.output_dir / "debug_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {results_path}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print_header("æµ‹è¯•æ€»ç»“")
        
        total = len(self.results["tests"])
        passed = sum(1 for t in self.results["tests"].values() if t.get("status") == "success")
        failed = sum(1 for t in self.results["tests"].values() if t.get("status") == "error")
        skipped = sum(1 for t in self.results["tests"].values() if t.get("status") == "skipped")
        
        print(f"\næ€»è®¡: {total} ä¸ªæµ‹è¯•")
        print(f"  âœ… é€šè¿‡: {passed}")
        print(f"  âŒ å¤±è´¥: {failed}")
        print(f"  â­ï¸ è·³è¿‡: {skipped}")
        
        if failed == 0:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åç«¯åŠŸèƒ½æ­£å¸¸ã€‚")
        else:
            print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        
        print(f"\nè¾“å‡ºæ–‡ä»¶ä½äº: {self.output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NeuroScan AI åç«¯è°ƒè¯•è„šæœ¬")
    parser.add_argument(
        "--with-segmentation",
        action="store_true",
        help="åŒ…å«åˆ†å‰²æµ‹è¯•ï¼ˆéœ€è¦ GPUï¼Œè€—æ—¶è¾ƒé•¿ï¼‰"
    )
    
    args = parser.parse_args()
    
    debugger = BackendDebugger()
    debugger.run_all_tests(skip_segmentation=not args.with_segmentation)


if __name__ == "__main__":
    main()
