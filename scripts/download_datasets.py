#!/usr/bin/env python3
"""
NeuroScan AI - å…¬å¼€åŒ»å­¦å½±åƒæ•°æ®é›†ä¸‹è½½å·¥å…·

æ”¯æŒçš„æ•°æ®é›†:
1. Learn2Reg Challenge (Lung CT) - è‚ºéƒ¨å¸æ°”/å‘¼æ°”é…å¯¹
2. RIDER Lung CT - è‚ºç™Œé‡å¤æ‰«æ
3. NSCLC Radiogenomics - è‚ºç™ŒåŸºå› ç»„å­¦
4. LIDC-IDRI - è‚ºç»“èŠ‚æ•°æ®é›†
5. Longitudinal CT (autoPET) - è‚¿ç˜¤çºµå‘éšè®¿

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_datasets.py --list           # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    python scripts/download_datasets.py --dataset learn2reg  # ä¸‹è½½æŒ‡å®šæ•°æ®é›†
    python scripts/download_datasets.py --all            # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
"""

import os
import sys
import json
import urllib.request
import urllib.parse
import zipfile
import tarfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, List

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


# ============ æ•°æ®é›†æ³¨å†Œè¡¨ ============

DATASETS = {
    "learn2reg": {
        "name": "Learn2Reg Lung CT",
        "description": "è‚ºéƒ¨å¸æ°”/å‘¼æ°”é…å¯¹ CTï¼ŒåŒ…å«æ˜¾è‘—è§£å‰–å½¢å˜ï¼Œé€‚åˆé…å‡†æµ‹è¯•",
        "size": "~300 MB",
        "format": "NIfTI",
        "source": "Zenodo",
        "url": "https://zenodo.org/api/records/3835682/files/training.zip/content",
        "license": "CC BY-NC 4.0",
        "pairs": 20,
        "recommended": True
    },
    "rider_lung": {
        "name": "RIDER Lung CT",
        "description": "32ä¾‹éå°ç»†èƒè‚ºç™Œæ‚£è€…çš„åŒæ—¥é‡å¤ CT æ‰«æ",
        "size": "~43 GB",
        "format": "DICOM",
        "source": "TCIA",
        "url": "https://wiki.cancerimagingarchive.net/display/Public/RIDER+Lung+CT",
        "license": "CC BY 3.0",
        "pairs": 32,
        "requires_nbia": True  # éœ€è¦ NBIA Data Retriever
    },
    "nlst_sample": {
        "name": "NLST Sample (è‚ºç™Œç­›æŸ¥è¯•éªŒæ ·æœ¬)",
        "description": "å›½å®¶è‚ºç™Œç­›æŸ¥è¯•éªŒçš„ç¤ºä¾‹æ•°æ®",
        "size": "~500 MB",
        "format": "DICOM",
        "source": "TCIA",
        "url": "https://www.cancerimagingarchive.net/collection/nlst/",
        "license": "TCIA Data Usage Policy",
        "requires_registration": True
    },
    "covid19_ct": {
        "name": "COVID-19 CT Scans",
        "description": "COVID-19 æ‚£è€…èƒ¸éƒ¨ CT æ‰«æå…¬å¼€æ•°æ®é›†",
        "size": "~2 GB",
        "format": "NIfTI/PNG",
        "source": "Kaggle",
        "url": "https://www.kaggle.com/datasets/plameneduardo/sarscov2-ctscan-dataset",
        "license": "CC BY-NC-SA 4.0"
    },
    "lungmask_sample": {
        "name": "LungMask Sample Data",
        "description": "ç”¨äºæµ‹è¯• lungmask åˆ†å‰²çš„ç¤ºä¾‹ CT æ•°æ®",
        "size": "~50 MB",
        "format": "NIfTI",
        "source": "GitHub",
        "url": "https://github.com/JoHof/lungmask",
        "license": "MIT"
    },
    "autopet_longitudinal": {
        "name": "autoPET Longitudinal CT",
        "description": "300ä¾‹é»‘è‰²ç´ ç˜¤æ‚£è€…çš„çºµå‘ CT æ•°æ®ï¼ˆåŸºçº¿+éšè®¿ï¼‰",
        "size": "~150 GB",
        "format": "NIfTI",
        "source": "FDAT",
        "url": "https://doi.org/10.57754/FDAT.qwsry-7t837",
        "license": "CC BY-NC 4.0",
        "pairs": 300,
        "external_download": True  # éœ€è¦æ‰‹åŠ¨ä¸‹è½½
    }
}


def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def setup_proxy():
    """è®¾ç½®ä»£ç†"""
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–ä»£ç†
    proxy = os.environ.get('http_proxy') or os.environ.get('https_proxy') or os.environ.get('HTTP_PROXY') or os.environ.get('HTTPS_PROXY')
    
    # é»˜è®¤ä»£ç†é…ç½®ï¼ˆå¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼‰
    if not proxy:
        # å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®é»˜è®¤ä»£ç†
        # proxy = "http://127.0.0.1:7890"  # ç¤ºä¾‹ï¼šæœ¬åœ°ä»£ç†
        pass
    
    if proxy:
        print(f"   ğŸŒ ä½¿ç”¨ä»£ç†: {proxy}")
        proxy_handler = urllib.request.ProxyHandler({
            'http': proxy,
            'https': proxy
        })
        opener = urllib.request.build_opener(proxy_handler)
        urllib.request.install_opener(opener)
        return True
    return False


def download_with_progress(url: str, output_path: str, description: str = "", use_proxy: bool = True) -> bool:
    """å¸¦è¿›åº¦æ¡çš„ä¸‹è½½"""
    print(f"\nâ¬‡ï¸  ä¸‹è½½: {description}")
    print(f"   URL: {url[:80]}...")
    
    # è®¾ç½®ä»£ç†
    if use_proxy:
        setup_proxy()
    
    def progress_hook(block_num, block_size, total_size):
        downloaded = block_num * block_size
        if total_size > 0:
            percent = min(100, downloaded * 100 / total_size)
            downloaded_mb = downloaded / (1024 * 1024)
            total_mb = total_size / (1024 * 1024)
            sys.stdout.write(f"\r   è¿›åº¦: {percent:.1f}% ({downloaded_mb:.1f}/{total_mb:.1f} MB)")
            sys.stdout.flush()
    
    try:
        # è®¾ç½®è¶…æ—¶å’Œé‡è¯•
        socket_timeout = 60  # 60ç§’è¶…æ—¶
        import socket
        socket.setdefaulttimeout(socket_timeout)
        
        urllib.request.urlretrieve(url, output_path, progress_hook)
        print("\n   âœ… ä¸‹è½½å®Œæˆ")
        return True
    except Exception as e:
        print(f"\n   âŒ ä¸‹è½½å¤±è´¥: {e}")
        print("   ğŸ’¡ æç¤º: å¯ä»¥è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡ http_proxy æˆ– https_proxy")
        return False


def extract_archive(archive_path: Path, extract_to: Path) -> bool:
    """è§£å‹å‹ç¼©åŒ…"""
    print(f"\nğŸ“‚ è§£å‹: {archive_path.name}")
    
    try:
        if archive_path.suffix == '.zip':
            with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                zip_ref.extractall(extract_to)
        elif archive_path.suffix in ['.tar', '.gz', '.tgz']:
            with tarfile.open(archive_path, 'r:*') as tar_ref:
                tar_ref.extractall(extract_to)
        else:
            print(f"   âš ï¸ ä¸æ”¯æŒçš„å‹ç¼©æ ¼å¼: {archive_path.suffix}")
            return False
        
        print(f"   âœ… è§£å‹å®Œæˆ: {extract_to}")
        return True
    except Exception as e:
        print(f"   âŒ è§£å‹å¤±è´¥: {e}")
        return False


def list_datasets():
    """åˆ—å‡ºå¯ç”¨æ•°æ®é›†"""
    print_header("å¯ç”¨æ•°æ®é›†")
    
    for key, info in DATASETS.items():
        recommended = " â­ æ¨è" if info.get("recommended") else ""
        external = " (éœ€æ‰‹åŠ¨ä¸‹è½½)" if info.get("external_download") else ""
        nbia = " (éœ€ NBIA)" if info.get("requires_nbia") else ""
        
        print(f"\nğŸ“¦ {key}{recommended}{external}{nbia}")
        print(f"   åç§°: {info['name']}")
        print(f"   æè¿°: {info['description']}")
        print(f"   å¤§å°: {info['size']}")
        print(f"   æ ¼å¼: {info['format']}")
        print(f"   æ¥æº: {info['source']}")
        print(f"   è®¸å¯: {info['license']}")
        if "pairs" in info:
            print(f"   é…å¯¹æ•°: {info['pairs']}")


def download_learn2reg():
    """ä¸‹è½½ Learn2Reg è‚ºéƒ¨ CT æ•°æ®"""
    print_header("ä¸‹è½½ Learn2Reg Lung CT")
    
    info = DATASETS["learn2reg"]
    raw_dir = PROJECT_ROOT / "data" / "raw"
    zip_path = raw_dir / "Learn2Reg_training.zip"
    
    raw_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¸‹è½½
    if not zip_path.exists():
        if not download_with_progress(info["url"], str(zip_path), info["name"]):
            return None
    else:
        print(f"   å‹ç¼©åŒ…å·²å­˜åœ¨: {zip_path}")
    
    # è§£å‹
    task_dir = raw_dir / "training"
    if not task_dir.exists():
        if not extract_archive(zip_path, raw_dir):
            return None
    
    # é…ç½®ç—…ä¾‹
    scans_dir = task_dir / "scans"
    masks_dir = task_dir / "lungMasks"
    
    configured = 0
    for case_id in range(1, 21):  # 20 ä¸ªç—…ä¾‹
        case_str = f"{case_id:03d}"
        inspiration = scans_dir / f"case_{case_str}_insp.nii.gz"
        expiration = scans_dir / f"case_{case_str}_exp.nii.gz"
        
        if inspiration.exists() and expiration.exists():
            case_dir = PROJECT_ROOT / "data" / "processed" / f"real_lung_{case_str}"
            case_dir.mkdir(parents=True, exist_ok=True)
            
            baseline = case_dir / "baseline.nii.gz"
            followup = case_dir / "followup.nii.gz"
            
            if not baseline.exists():
                shutil.copy(inspiration, baseline)
            if not followup.exists():
                shutil.copy(expiration, followup)
            
            # å¤åˆ¶æ©ç 
            baseline_mask_src = masks_dir / f"case_{case_str}_insp.nii.gz"
            followup_mask_src = masks_dir / f"case_{case_str}_exp.nii.gz"
            
            if baseline_mask_src.exists():
                baseline_mask = case_dir / "baseline_mask.nii.gz"
                if not baseline_mask.exists():
                    shutil.copy(baseline_mask_src, baseline_mask)
            
            if followup_mask_src.exists():
                followup_mask = case_dir / "followup_mask.nii.gz"
                if not followup_mask.exists():
                    shutil.copy(followup_mask_src, followup_mask)
            
            configured += 1
            print(f"   âœ… Case {case_str}: baseline + followup + masks")
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "dataset": info["name"],
        "source": info["source"],
        "license": info["license"],
        "download_date": datetime.now().isoformat(),
        "configured_pairs": configured,
        "description": "åŒä¸€æ‚£è€…çš„å¸æ°”æœ«å’Œå‘¼æ°”æœ« CT æ‰«æ"
    }
    
    metadata_path = PROJECT_ROOT / "data" / "processed" / "learn2reg_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Learn2Reg æ•°æ®å‡†å¤‡å®Œæˆ!")
    print(f"   é…ç½®çš„ç—…ä¾‹æ•°: {configured}")
    print(f"   æ•°æ®ä½ç½®: {PROJECT_ROOT / 'data' / 'processed'}")
    
    return PROJECT_ROOT / "data" / "processed"


def download_sample_nibabel():
    """ä¸‹è½½ NiBabel ç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"""
    print_header("ä¸‹è½½ NiBabel ç¤ºä¾‹æ•°æ®")
    
    import nibabel as nib
    from nibabel import testing
    
    sample_dir = PROJECT_ROOT / "data" / "raw" / "nibabel_samples"
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å– nibabel æµ‹è¯•æ•°æ®
    example_files = [
        "anatomical.nii",
        "example4d.nii.gz"
    ]
    
    for filename in example_files:
        src = Path(testing.data_path) / filename
        if src.exists():
            dst = sample_dir / filename
            if not dst.exists():
                shutil.copy(src, dst)
                print(f"   âœ… {filename}")
    
    print(f"\nâœ… ç¤ºä¾‹æ•°æ®ä¿å­˜è‡³: {sample_dir}")
    return sample_dir


def generate_synthetic_longitudinal(n_cases: int = 5):
    """ç”Ÿæˆåˆæˆçš„çºµå‘ CT æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    print_header("ç”Ÿæˆåˆæˆçºµå‘æ•°æ®")
    
    import numpy as np
    import nibabel as nib
    
    output_dir = PROJECT_ROOT / "data" / "processed" / "synthetic"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for case_id in range(1, n_cases + 1):
        case_dir = output_dir / f"case_{case_id:03d}"
        case_dir.mkdir(exist_ok=True)
        
        # ç”ŸæˆåŸºçº¿å›¾åƒ
        shape = (128, 128, 64)
        baseline = np.random.randn(*shape).astype(np.float32) * 100 - 500
        
        # æ·»åŠ ä¸€äº›ç»“æ„
        x, y, z = np.mgrid[:shape[0], :shape[1], :shape[2]]
        center = np.array(shape) // 2
        
        # æ·»åŠ  "è‚ºéƒ¨" åŒºåŸŸ
        lung_mask = ((x - center[0])**2 / 40**2 + 
                     (y - center[1])**2 / 35**2 + 
                     (z - center[2])**2 / 25**2) < 1
        baseline[lung_mask] = -800 + np.random.randn(*baseline[lung_mask].shape) * 50
        
        # æ·»åŠ  "ç»“èŠ‚"
        nodule_center = center + np.array([10, 10, 5])
        nodule_mask = ((x - nodule_center[0])**2 + 
                       (y - nodule_center[1])**2 + 
                       (z - nodule_center[2])**2) < 8**2
        baseline[nodule_mask] = 50 + np.random.randn(*baseline[nodule_mask].shape) * 20
        
        # ç”Ÿæˆéšè®¿å›¾åƒï¼ˆæ¨¡æ‹Ÿå˜åŒ–ï¼‰
        followup = baseline.copy()
        # ç»“èŠ‚ç•¥å¾®å¢å¤§
        nodule_mask_2 = ((x - nodule_center[0])**2 + 
                         (y - nodule_center[1])**2 + 
                         (z - nodule_center[2])**2) < 10**2
        followup[nodule_mask_2] = 55 + np.random.randn(*followup[nodule_mask_2].shape) * 20
        
        # ä¿å­˜
        affine = np.eye(4)
        
        baseline_img = nib.Nifti1Image(baseline, affine)
        baseline_img.header.set_zooms((1.5, 1.5, 2.0))
        nib.save(baseline_img, case_dir / "baseline.nii.gz")
        
        followup_img = nib.Nifti1Image(followup, affine)
        followup_img.header.set_zooms((1.5, 1.5, 2.0))
        nib.save(followup_img, case_dir / "followup.nii.gz")
        
        print(f"   âœ… Case {case_id:03d}: baseline + followup")
    
    print(f"\nâœ… åˆæˆæ•°æ®ç”Ÿæˆå®Œæˆ: {output_dir}")
    return output_dir


def show_tcia_instructions():
    """æ˜¾ç¤º TCIA æ•°æ®é›†ä¸‹è½½è¯´æ˜"""
    print_header("TCIA æ•°æ®é›†ä¸‹è½½è¯´æ˜")
    
    print("""
The Cancer Imaging Archive (TCIA) æ•°æ®é›†éœ€è¦ä½¿ç”¨ä¸“é—¨çš„å·¥å…·ä¸‹è½½ã€‚

ğŸ“‹ ä¸‹è½½æ­¥éª¤:

1. å®‰è£… NBIA Data Retriever
   ä¸‹è½½åœ°å€: https://wiki.cancerimagingarchive.net/display/NBIA/Downloading+TCIA+Images
   
2. è®¿é—®æ•°æ®é›†é¡µé¢è·å– .tcia manifest æ–‡ä»¶
   - RIDER Lung CT: https://www.cancerimagingarchive.net/collection/rider-lung-ct/
   - NLST: https://www.cancerimagingarchive.net/collection/nlst/
   
3. ä½¿ç”¨ NBIA Data Retriever æ‰“å¼€ .tcia æ–‡ä»¶è¿›è¡Œä¸‹è½½

4. ä¸‹è½½å®Œæˆåï¼Œå°† DICOM æ–‡ä»¶æ”¾å…¥:
   {}/data/raw/tcia_<dataset_name>/

5. ä½¿ç”¨ NeuroScan AI çš„ DICOM åŠ è½½å™¨å¤„ç†æ•°æ®
""".format(PROJECT_ROOT))


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="NeuroScan AI æ•°æ®é›†ä¸‹è½½å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python download_datasets.py --list              # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
  python download_datasets.py --dataset learn2reg # ä¸‹è½½ Learn2Reg æ•°æ®
  python download_datasets.py --synthetic 10      # ç”Ÿæˆ 10 ä¸ªåˆæˆç—…ä¾‹
  python download_datasets.py --tcia-help         # æ˜¾ç¤º TCIA ä¸‹è½½è¯´æ˜
        """
    )
    
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨æ•°æ®é›†")
    parser.add_argument("--dataset", choices=list(DATASETS.keys()), help="è¦ä¸‹è½½çš„æ•°æ®é›†")
    parser.add_argument("--synthetic", type=int, metavar="N", help="ç”Ÿæˆ N ä¸ªåˆæˆç—…ä¾‹")
    parser.add_argument("--nibabel-sample", action="store_true", help="ä¸‹è½½ NiBabel ç¤ºä¾‹æ•°æ®")
    parser.add_argument("--tcia-help", action="store_true", help="æ˜¾ç¤º TCIA ä¸‹è½½è¯´æ˜")
    parser.add_argument("--all", action="store_true", help="ä¸‹è½½æ‰€æœ‰è‡ªåŠ¨ä¸‹è½½çš„æ•°æ®é›†")
    
    args = parser.parse_args()
    
    if args.list:
        list_datasets()
        return
    
    if args.tcia_help:
        show_tcia_instructions()
        return
    
    results = {}
    
    if args.dataset == "learn2reg" or args.all:
        results["learn2reg"] = download_learn2reg()
    
    if args.nibabel_sample or args.all:
        results["nibabel_sample"] = download_sample_nibabel()
    
    if args.synthetic:
        results["synthetic"] = generate_synthetic_longitudinal(args.synthetic)
    
    if not any([args.list, args.dataset, args.synthetic, args.nibabel_sample, 
                args.tcia_help, args.all]):
        parser.print_help()
        return
    
    # æ‰“å°æ€»ç»“
    if results:
        print_header("ä¸‹è½½æ€»ç»“")
        for name, path in results.items():
            status = "âœ…" if path else "âŒ"
            print(f"  {status} {name}: {path or 'å¤±è´¥'}")


if __name__ == "__main__":
    main()
